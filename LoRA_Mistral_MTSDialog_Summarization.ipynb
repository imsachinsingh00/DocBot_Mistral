{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5addbab",
      "metadata": {
        "id": "d5addbab"
      },
      "source": [
        "# LoRA Fine-tuning and Inference on Mistral-7B for Medical Dialogue Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7881c0dc",
      "metadata": {
        "id": "7881c0dc",
        "outputId": "2ae87dc0-98f2-452c-da93-e67b2508fe89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Set CUDA device manually if needed\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f77b99",
      "metadata": {
        "id": "e5f77b99",
        "outputId": "68557600-6e52-4055-dfc6-dcde5ced3813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# A) Pick the string for device_map\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        chosen_cuda_str = \"cuda:1\"\n",
        "    else:\n",
        "        chosen_cuda_str = \"cuda:0\"\n",
        "else:\n",
        "    chosen_cuda_str = \"cpu\"\n",
        "\n",
        "# B) Then create the actual torch.device\n",
        "device = torch.device(chosen_cuda_str)\n",
        "print(f\">>> Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f5f184",
      "metadata": {
        "id": "c0f5f184"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "csv_path = \"https://raw.githubusercontent.com/abachaa/MTS-Dialog/refs/heads/main/Main-Dataset/MTS-Dialog-TrainingSet.csv\"  # ← e.g. \"./data/my_dialogue_summary.csv\"\n",
        "\n",
        "# columns: [ \"ID\", \"section_header\", \"section_text\" (the reference), \"dialogue\" ]\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# For simplicity, we’ll keep exactly these 4 columns.\n",
        "df = df[[\"ID\", \"section_header\", \"section_text\", \"dialogue\"]]\n",
        "\n",
        "# Train/Val split\n",
        "train_frac = 0.9\n",
        "train_df = df.sample(frac=train_frac, random_state=42).reset_index(drop=True)\n",
        "val_df   = df.drop(train_df.index).reset_index(drop=True)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset   = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Put them into a DatasetDict for convenience\n",
        "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc6ef24",
      "metadata": {
        "id": "0fc6ef24",
        "outputId": "11a3e5e6-83ab-43c9-cd25-dd7e35937788"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.48s/it]\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA params: 3,407,872  /  Total params: 3,755,487,232 → trainable = 0.09%\n"
          ]
        }
      ],
      "source": [
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# 1. Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "# 2. Load base Mistral in 4-bit + CPU offload (to reduce GPU VRAM usage)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map={\"\": chosen_cuda_str},\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,   # Mistral uses custom code\n",
        ")\n",
        "\n",
        "# 3. Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                    # LoRA rank\n",
        "    lora_alpha=32,          # LoRA alpha\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # fine-tune Q/K/V or Q/V\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# 4. Apply LoRA\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "\n",
        "# 5. Count trainable params\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"LoRA params: {trainable:,}  /  Total params: {total:,} → trainable = {100 * trainable/total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafa9712",
      "metadata": {
        "id": "eafa9712",
        "outputId": "7ecb6c26-e1e3-4b78-8179-6a34a84dab69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1081/1081 [00:04<00:00, 259.37 examples/s]\n",
            "Map: 100%|██████████| 120/120 [00:00<00:00, 253.68 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing\n",
        "\n",
        "max_source_length = 512\n",
        "max_target_length = 256\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    For each example, we will:\n",
        "      - Concatenate the prompt prefix + section_header + dialogue\n",
        "      - Tokenize them (truncating/padding up to max_source_length)\n",
        "      - Tokenize the reference summary (truncating/padding up to max_target_length)\n",
        "      - Store reference ids in 'labels' (with padding tokens replaced by -100).\n",
        "    \"\"\"\n",
        "    inputs = []\n",
        "    labels = []\n",
        "\n",
        "    for sh, dlg, ref in zip(examples[\"section_header\"], examples[\"dialogue\"], examples[\"section_text\"]):\n",
        "        # 1. Build the prompt prefix\n",
        "        prompt = f\"Summarize the following dialogue for section: {sh}\\n{dlg}\\nSummary:\"\n",
        "\n",
        "        # 2. Tokenize prompt\n",
        "        tokenized_inputs = tokenizer(\n",
        "            prompt,\n",
        "            max_length=max_source_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        # 3. Tokenize the reference (section_text)\n",
        "        tokenized_labels = tokenizer(\n",
        "            ref,\n",
        "            max_length=max_target_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        # 4. Replace pad token id’s in labels with -100 so they’re ignored in loss\n",
        "        label_ids = [\n",
        "            (tok if tok != tokenizer.pad_token_id else -100)\n",
        "            for tok in tokenized_labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "        inputs.append(tokenized_inputs[\"input_ids\"])\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    # Return a dict with input_ids, attention_mask, and labels\n",
        "    batch = {\n",
        "        \"input_ids\": inputs,\n",
        "        \"attention_mask\": [\n",
        "            [1 if id != tokenizer.pad_token_id else 0 for id in seq] for seq in inputs\n",
        "        ],\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Apply preprocessing (batched)\n",
        "tokenized_datasets = dataset_dict.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_dict[\"train\"].column_names,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d636942",
      "metadata": {
        "id": "8d636942",
        "outputId": "a13f04a4-3f87-4915-a01f-7b48422d9468"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "C:\\Users\\singh\\AppData\\Local\\Temp\\3\\ipykernel_6544\\386793575.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,            # because Mistral is a causal LM\n",
        "    pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-mts-summary_1\",   # where to store checkpoints + logs\n",
        "    per_device_train_batch_size=4,         # adjust per your GPU VRAM\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,         # effectively BATCH_SIZE = 4 × 4 = 16\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=3e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"epoch\",           # run evaluation each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",                      # disable WandB/other reporting\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,           # needed for Seq2Seq mode\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7926cb",
      "metadata": {
        "id": "5d7926cb"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bleu_metric  = evaluate.load(\"bleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    eval_preds is a tuple (predictions, labels).\n",
        "    - predictions: model.logits -> we take argmax to get token IDs\n",
        "    - labels: already preprocessed with -100 for padding\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "    if isinstance(predictions, tuple):\n",
        "        preds_ids = predictions[0].argmax(-1)\n",
        "    else:\n",
        "        preds_ids = predictions.argmax(-1)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result_rouge = rouge_metric.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "\n",
        "    result_bleu = bleu_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[ref] for ref in decoded_labels]\n",
        "    )\n",
        "\n",
        "    result = {\n",
        "        \"rouge1\": result_rouge[\"rouge1\"].mid.fmeasure,\n",
        "        \"rouge2\": result_rouge[\"rouge2\"].mid.fmeasure,\n",
        "        \"rougeL\": result_rouge[\"rougeL\"].mid.fmeasure,\n",
        "        \"bleu\":   result_bleu[\"bleu\"],\n",
        "    }\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "trainer.compute_metrics = compute_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf341bf",
      "metadata": {
        "id": "8bf341bf",
        "outputId": "6056195c-e898-465c-f7dc-69765bdf4327"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='201' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [201/201 17:19, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.398000</td>\n",
              "      <td>1.150830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.257200</td>\n",
              "      <td>1.032884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.090900</td>\n",
              "      <td>0.991367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\singh\\.conda\\envs\\myenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./mistral-mts-summary_1\\\\tokenizer_config.json',\n",
              " './mistral-mts-summary_1\\\\special_tokens_map.json',\n",
              " './mistral-mts-summary_1\\\\tokenizer.json')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# training\n",
        "trainer.train()\n",
        "\n",
        "# After training, save the LoRA adapter & tokenizer\n",
        "model.save_pretrained(\"./mistral-mts-summary_1\")\n",
        "tokenizer.save_pretrained(\"./mistral-mts-summary_1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed67b779",
      "metadata": {
        "id": "ed67b779"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on validation set\n",
        "def evaluate_in_batches(model, tokenized_dataset, tokenizer, batch_size=1):\n",
        "    \"\"\"\n",
        "    Generates summaries for each example in tokenized_dataset (which already\n",
        "    contains input_ids, attention_mask, and labels). Returns a dict of metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds  = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(tokenized_dataset), batch_size):\n",
        "        batch = tokenized_dataset.select(range(i, min(i + batch_size, len(tokenized_dataset))))\n",
        "        for item in batch:\n",
        "            input_ids = torch.tensor([item[\"input_ids\"]]).to(model.device)\n",
        "            attention_mask = torch.tensor([item[\"attention_mask\"]]).to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_ids = model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=200,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True,\n",
        "                )\n",
        "\n",
        "            pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "            all_preds.append(pred)\n",
        "\n",
        "            label_ids = [tok for tok in item[\"labels\"] if tok != -100]\n",
        "            ref = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
        "            all_labels.append(ref)\n",
        "\n",
        "\n",
        "    rouge_res = rouge_metric.compute(predictions=all_preds, references=all_labels)\n",
        "\n",
        "    bleu_res  = bleu_metric.compute(\n",
        "        predictions=all_preds, references=[[r] for r in all_labels]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"ROUGE-1\": rouge_res[\"rouge1\"],\n",
        "        \"ROUGE-2\": rouge_res[\"rouge2\"],\n",
        "        \"ROUGE-L\": rouge_res[\"rougeL\"],\n",
        "        \"BLEU\":    bleu_res[\"bleu\"],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f74b9d",
      "metadata": {
        "id": "12f74b9d",
        "outputId": "8e4fee3a-56b7-426b-c0d7-a5db8cf93de5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final validation metrics:\n",
            "  ROUGE-1: 0.1318\n",
            "  ROUGE-2: 0.0456\n",
            "  ROUGE-L: 0.0900\n",
            "  BLEU:    0.0260\n"
          ]
        }
      ],
      "source": [
        "validation_metrics = evaluate_in_batches(\n",
        "    model,\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    tokenizer,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"Final validation metrics:\")\n",
        "print(f\"  ROUGE-1: {validation_metrics['ROUGE-1']:.4f}\")\n",
        "print(f\"  ROUGE-2: {validation_metrics['ROUGE-2']:.4f}\")\n",
        "print(f\"  ROUGE-L: {validation_metrics['ROUGE-L']:.4f}\")\n",
        "print(f\"  BLEU:    {validation_metrics['BLEU']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c82176",
      "metadata": {
        "id": "d2c82176",
        "outputId": "f67602d5-7105-4de6-e82e-cd3558019226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Pick the string for device_map\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        chosen_cuda_str = \"cuda:1\"\n",
        "    else:\n",
        "        chosen_cuda_str = \"cuda:0\"\n",
        "else:\n",
        "    chosen_cuda_str = \"cpu\"\n",
        "\n",
        "device = torch.device(chosen_cuda_str)\n",
        "print(f\">>> Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ccdbcd",
      "metadata": {
        "id": "e9ccdbcd",
        "outputId": "5e494dd8-003e-4625-bf95-118ab28b35b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.63s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FINAL PROMPT ===\n",
            "\n",
            "Example 1:\n",
            "Dialogue:\n",
            "Doctor: Hello, Mrs. Smith. What seems to be troubling you today?\n",
            "Patient: I’ve been having shortness of breath and a mild cough for two weeks.\n",
            "Doctor: Any history of asthma or allergies?\n",
            "Patient: No, I’ve never had any breathing problems before.\n",
            "Summary:\n",
            "The patient, a middle-aged woman, presented with a two-week history of shortness of breath and mild cough without prior respiratory conditions. The physician asked about asthma/allergies, which the patient denied.\n",
            "\n",
            "Example 2:\n",
            "Dialogue:\n",
            "Doctor: Good morning. How are you feeling since your last visit?\n",
            "Patient: I still have a sharp pain in my right knee when I climb stairs.\n",
            "Doctor: Does the pain radiate anywhere else?\n",
            "Patient: No, it’s just in my knee. It started about a month ago.\n",
            "Summary:\n",
            "The patient continues to experience sharp knee pain exacerbated by stair climbing for one month, localized to the right knee with no radiation.\n",
            "\n",
            "\n",
            "Now you:\n",
            "Summarize the following dialogue for section: GENHX\n",
            "\n",
            "Doctor: What brings you back into the clinic today, miss?\n",
            "Patient: I've had chest pain for the last few days.\n",
            "Doctor: When did it start?\n",
            "\n",
            "Summary:\n",
            "\n",
            "====================\n",
            "\n",
            "📝 Generated Summary:\n",
            " Chest pain of unknown etiology started two days ago and is worsened by exertion. Patient denies any other symptoms such as nausea, vomiting, diaphoresis, or palpitations.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        chosen_cuda_str = \"cuda:1\"\n",
        "    else:\n",
        "        chosen_cuda_str = \"cuda:0\"\n",
        "else:\n",
        "    chosen_cuda_str = \"cpu\"\n",
        "\n",
        "device = torch.device(chosen_cuda_str)\n",
        "hf_token = os.getenv(\"HF_TOKEN\", None)\n",
        "\n",
        "model_name    = \"mistralai/Mistral-7B-v0.1\"\n",
        "lora_save_dir = \"./mistral-mts-summary_1\"\n",
        "\n",
        "if hf_token:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "if hf_token:\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        device_map={\"\": chosen_cuda_str},\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=hf_token,\n",
        "    )\n",
        "else:\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        device_map={\"\": chosen_cuda_str},\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "base.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = PeftModel.from_pretrained(base, lora_save_dir)\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "few_shot_prompts = \"\"\"\n",
        "Example 1:\n",
        "Dialogue:\n",
        "Doctor: Hello, Mrs. Smith. What seems to be troubling you today?\n",
        "Patient: I’ve been having shortness of breath and a mild cough for two weeks.\n",
        "Doctor: Any history of asthma or allergies?\n",
        "Patient: No, I’ve never had any breathing problems before.\n",
        "Summary:\n",
        "The patient, a middle-aged woman, presented with a two-week history of shortness of breath and mild cough without prior respiratory conditions. The physician asked about asthma/allergies, which the patient denied.\n",
        "\n",
        "Example 2:\n",
        "Dialogue:\n",
        "Doctor: Good morning. How are you feeling since your last visit?\n",
        "Patient: I still have a sharp pain in my right knee when I climb stairs.\n",
        "Doctor: Does the pain radiate anywhere else?\n",
        "Patient: No, it’s just in my knee. It started about a month ago.\n",
        "Summary:\n",
        "The patient continues to experience sharp knee pain exacerbated by stair climbing for one month, localized to the right knee with no radiation.\n",
        "\"\"\"\n",
        "\n",
        "new_dialogue_header = \"GENHX\"\n",
        "new_dialogue_text = \"\"\"\n",
        "Doctor: What brings you back into the clinic today, miss?\n",
        "Patient: I've had chest pain for the last few days.\n",
        "Doctor: When did it start?\n",
        "\"\"\"\n",
        "\n",
        "inference_prompt = few_shot_prompts + f\"\"\"\n",
        "\n",
        "Now you:\n",
        "Summarize the following dialogue for section: {new_dialogue_header}\n",
        "{new_dialogue_text}\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== FINAL PROMPT ===\")\n",
        "print(inference_prompt)\n",
        "print(\"====================\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    inference_prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=1024,\n",
        ")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        min_length=30,\n",
        "        length_penalty=0.8,\n",
        "        no_repeat_ngram_size=2,\n",
        "    )\n",
        "\n",
        "full_out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "if \"Summary:\" in full_out:\n",
        "    generated_summary = full_out.rsplit(\"Summary:\", 1)[-1].strip()\n",
        "else:\n",
        "    generated_summary = full_out\n",
        "\n",
        "print(\"\\n📝 Generated Summary:\\n\", generated_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b5e43b",
      "metadata": {
        "id": "84b5e43b",
        "outputId": "84b38552-ba05-4ec0-ff8d-6415d3b9001f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example A Generated Summary:\n",
            " This is a case of influenza with fever, cough, and myalgia. The patient also has a history of asthma and hypertension. He has not been vaccinated against the flu this year.\n",
            "\n",
            "Example B Generated Summary:\n",
            " Diabetes mellitus, type 2\n",
            "Poorly controlled blood sugars\n",
            "Dietary indiscretion\n",
            "Missed medications\n",
            "Lightheadedness\n",
            "Excessive Thirst\n",
            "\n",
            "Guest_clinician: Hello, Doctor. I'm the nurse practition\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------\n",
        "# Example A (Influenza Suspect Dialogue)\n",
        "# ---------------------------------------------\n",
        "exampleA = \"\"\"\n",
        "Doctor: Hello, Mr. Patel. Are you having any fever or chills?\n",
        "Patient: Yes, I’ve had a 102°F fever since yesterday and chills last night.\n",
        "Doctor: Any cough or stuffy nose?\n",
        "Patient: Mild cough and some congestion.\n",
        "Doctor: Do you have body aches?\n",
        "Patient: Yes, I feel sore all over.\n",
        "Summary:\n",
        "\"\"\"\n",
        "inputsA = tokenizer(exampleA, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "inputsA = {k: v.to(device) for k,v in inputsA.items()}\n",
        "with torch.no_grad():\n",
        "    outA = model.generate(\n",
        "        **inputsA,\n",
        "        max_new_tokens=60,\n",
        "        num_beams=4,\n",
        "        min_length=30,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "decodedA = tokenizer.decode(outA[0], skip_special_tokens=True)\n",
        "summA = decodedA.rsplit(\"Summary:\", 1)[-1].strip()\n",
        "print(\"\\nExample A Generated Summary:\\n\", summA)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Example B (Diabetes Follow-Up Dialogue)\n",
        "# ---------------------------------------------\n",
        "exampleB = \"\"\"\n",
        "Doctor: Good afternoon, Ms. Lee. How are your blood sugar readings?\n",
        "Patient: My fasting glucose has been around 180 mg/dL for the past week.\n",
        "Doctor: Have you changed your diet or medication?\n",
        "Patient: I missed two doses of metformin last week and ate more carbs.\n",
        "Doctor: Any dizziness or excessive thirst?\n",
        "Patient: Yes, I’m thirsty and lightheaded sometimes.\n",
        "Summary:\n",
        "\"\"\"\n",
        "inputsB = tokenizer(exampleB, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "inputsB = {k: v.to(device) for k,v in inputsB.items()}\n",
        "with torch.no_grad():\n",
        "    outB = model.generate(\n",
        "        **inputsB,\n",
        "        max_new_tokens=60,\n",
        "        num_beams=4,\n",
        "        min_length=30,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "decodedB = tokenizer.decode(outB[0], skip_special_tokens=True)\n",
        "summB = decodedB.rsplit(\"Summary:\", 1)[-1].strip()\n",
        "print(\"\\nExample B Generated Summary:\\n\", summB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0639a5af",
      "metadata": {
        "id": "0639a5af"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}